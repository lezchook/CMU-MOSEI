{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from warnings import simplefilter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('Data_Train_modified.csv')\n",
    "data_val = pd.read_csv('Data_Val_modified.csv')\n",
    "data_test = pd.read_csv('Data_Test_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "data_train['sentiment'] = data_train['sentiment'].progress_apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutaral')\n",
    "data_val['sentiment'] = data_val['sentiment'].progress_apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutaral')\n",
    "data_test['sentiment'] = data_test['sentiment'].progress_apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutaral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_count = data_train[data_train[\"sentiment\"] == \"neutaral\"].shape[0]\n",
    "data_positive = data_train[data_train[\"sentiment\"] == \"positive\"]\n",
    "data_negative = data_train[data_train[\"sentiment\"] == \"negative\"]\n",
    "\n",
    "data_positive_downsampled = resample(data_positive, replace=False, n_samples=neutral_count, random_state=42)\n",
    "data_negative_downsampled = resample(data_negative, replace=False, n_samples=neutral_count, random_state=42)\n",
    "data_train = pd.concat([data_train[data_train[\"sentiment\"] == \"neutaral\"], data_positive_downsampled, data_negative_downsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.reset_index().drop(columns=[\"index\"])\n",
    "data_val = data_val.reset_index().drop(columns=[\"index\"])\n",
    "data_test = data_test.reset_index().drop(columns=[\"index\"])\n",
    "\n",
    "data_train = data_train.rename(columns={\"video\": \"audio_file\"})\n",
    "data_val = data_val.rename(columns={\"video\": \"audio_file\"})\n",
    "data_test = data_test.rename(columns={\"video\": \"audio_file\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folder = \"./Audio/WAV_16000/\"\n",
    "SAMPLE_RATE = 16000\n",
    "HOP_LENGTH = 512\n",
    "N_FFT = 2048\n",
    "N_MFCC= 13\n",
    "MAX_TIME = int(np.array(data_train[\"end_time\"] - data_train[\"start_time\"]).mean())\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path, start_time, end_time):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    start_sample = int(start_time * sr)\n",
    "    end_sample = int(end_time * sr)\n",
    "    y_segment = y[start_sample:end_sample]\n",
    "        \n",
    "    mfcc = librosa.feature.mfcc(y=y_segment, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mfcc=N_MFCC)\n",
    "    mfcc = mfcc.T\n",
    "    max_frames = int(MAX_TIME * sr / HOP_LENGTH)\n",
    "\n",
    "    if len(mfcc) < max_frames:\n",
    "        pad_width = max_frames - len(mfcc)\n",
    "        mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:max_frames, :]\n",
    "    \n",
    "    return mfcc\n",
    "\n",
    "def extract_bert_features(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=600).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(data):\n",
    "    data[\"mfcc\"] = data.progress_apply(lambda row: extract_mfcc(os.path.join(audio_folder, row[\"audio_file\"] + \".wav\"), row[\"start_time\"], row[\"end_time\"]), axis=1)\n",
    "    return data\n",
    "\n",
    "def get_bert_embedings(data):\n",
    "    simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "    data[\"bert_features\"] = data[\"text\"].progress_apply(lambda x: extract_bert_features(x))\n",
    "    bert_columns = [f\"bert_{i}\" for i in range(768)]\n",
    "    data[bert_columns] = pd.DataFrame(data[\"bert_features\"].tolist(), index=data.index)\n",
    "    data.drop(columns=[\"bert_features\"], inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_train = get_mfcc(data_train.copy())\n",
    "audio_val = get_mfcc(data_val.copy())\n",
    "audio_test = get_mfcc(data_test.copy())\n",
    "\n",
    "text_train = get_bert_embedings(data_train.copy())\n",
    "text_val = get_bert_embedings(data_val.copy())\n",
    "text_test = get_bert_embedings(data_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_train = pd.concat([audio_train, pd.get_dummies(audio_train['sentiment'])], axis=1)\n",
    "audio_val = pd.concat([audio_val, pd.get_dummies(audio_val['sentiment'])], axis=1)\n",
    "audio_test = pd.concat([audio_test, pd.get_dummies(audio_test['sentiment'])], axis=1)\n",
    "\n",
    "text_train = pd.concat([text_train, pd.get_dummies(text_train['sentiment'])], axis=1)\n",
    "text_val = pd.concat([text_val, pd.get_dummies(text_val['sentiment'])], axis=1)\n",
    "text_test = pd.concat([text_test, pd.get_dummies(text_test['sentiment'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_audio_train = audio_train.drop(['audio_file', 'start_time', 'end_time', 'sentiment', 'happy', 'sad',\t'anger', 'surprise', 'disgust',\t'fear', 'text', 'ASR', 'negative', 'neutaral', 'positive'], axis=1)\n",
    "y_audio_train = audio_train[['negative', 'neutaral', 'positive']]\n",
    "\n",
    "X_audio_val = audio_val.drop(['audio_file', 'start_time', 'end_time', 'sentiment', 'happy', 'sad',\t'anger', 'surprise', 'disgust',\t'fear', 'text', 'ASR', 'negative', 'neutaral', 'positive'], axis=1)\n",
    "y_audio_val = audio_val[['negative', 'neutaral', 'positive']]\n",
    "\n",
    "X_audio_test = audio_test.drop(['audio_file', 'start_time', 'end_time', 'sentiment', 'happy', 'sad',\t'anger', 'surprise', 'disgust',\t'fear', 'text', 'ASR', 'negative', 'neutaral', 'positive'], axis=1)\n",
    "y_audio_test = audio_test[['negative', 'neutaral', 'positive']]\n",
    "\n",
    "X_text_train = text_train.drop(['audio_file', 'start_time', 'end_time', 'sentiment', 'happy', 'sad',\t'anger', 'surprise', 'disgust',\t'fear', 'text', 'ASR', 'negative', 'neutaral', 'positive'], axis=1)\n",
    "y_text_train = text_train[['negative', 'neutaral', 'positive']]\n",
    "\n",
    "X_text_val = text_val.drop(['audio_file', 'start_time', 'end_time', 'sentiment', 'happy', 'sad',\t'anger', 'surprise', 'disgust',\t'fear', 'text', 'ASR', 'negative', 'neutaral', 'positive'], axis=1)\n",
    "y_text_val = text_val[['negative', 'neutaral', 'positive']]\n",
    "\n",
    "X_text_test = text_test.drop(['audio_file', 'start_time', 'end_time', 'sentiment', 'happy', 'sad',\t'anger', 'surprise', 'disgust',\t'fear', 'text', 'ASR', 'negative', 'neutaral', 'positive'], axis=1)\n",
    "y_text_test = text_test[['negative', 'neutaral', 'positive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs_text, input_audio, targets, device):\n",
    "        self.inputs_text = torch.tensor(inputs_text, dtype=torch.float32).to(device)\n",
    "        self.inputs_audio = torch.tensor(input_audio, dtype=torch.float32).to(device)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs_text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs_text[index], self.inputs_audio[index], self.targets[index]\n",
    "\n",
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(2, 2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(2, 2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(2, 2)\n",
    "\n",
    "        self.linear1 = nn.Linear(input_shape[1] // 8 * 64, 128)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        return x\n",
    "\n",
    "class TransformerAudioClassifier(nn.Module):\n",
    "    def __init__(self, seq_len, input_dim, num_classes, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Linear(input_dim, 128)\n",
    "        self.positional_encoding = nn.Parameter(torch.rand(seq_len, 1, 128))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=nhead, dim_feedforward=512)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = x + self.positional_encoding\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)\n",
    "        return x\n",
    "\n",
    "class MultimodalSentimentClassifier(nn.Module):\n",
    "    def __init__(self, text_model, audio_model, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.text_model = text_model\n",
    "        self.audio_model = audio_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 + 128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, text_input, audio_input):\n",
    "        text_feat = self.text_model.extract_features(text_input)\n",
    "        audio_feat = self.audio_model.extract_features(audio_input)\n",
    "        combined = torch.cat((text_feat, audio_feat), dim=1)\n",
    "        out = self.classifier(combined)\n",
    "        return out      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "trainset = CustomDataset(X_text_train.to_numpy(), X_audio_train[\"mfcc\"], y_text_train.to_numpy(), device)\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "\n",
    "valset = CustomDataset(X_text_val.to_numpy(), X_audio_val[\"mfcc\"], y_text_val.to_numpy(), device)\n",
    "valloader = DataLoader(valset, batch_size=256, shuffle=False)\n",
    "\n",
    "testset = CustomDataset(X_text_test.to_numpy(), X_audio_test[\"mfcc\"], y_text_test.to_numpy(), device)\n",
    "testloader = DataLoader(testset, batch_size=256, shuffle=False)\n",
    "\n",
    "text_model = CNNTextClassifier(input_shape=X_text_train.shape, num_classes=y_text_train.shape[1])\n",
    "audio_model = TransformerAudioClassifier(seq_len=X_audio_train['mfcc'][0].shape[0], input_dim=13, num_classes=3)\n",
    "model = MultimodalSentimentClassifier(text_model, audio_model, num_classes=3).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data in trainloader:\n",
    "        X_text, X_audio, y = data\n",
    "        X_text = X_text.unsqueeze(1)\n",
    "        X_audio = X_audio.permute(1, 0, 2)\n",
    "        output = model(X_text, X_audio)\n",
    "        loss = loss_function(output, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(trainloader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            X_text, X_audio, y = data\n",
    "            X_text = X_text.unsqueeze(1)\n",
    "            X_audio = X_audio.permute(1, 0, 2)\n",
    "            output = model(X_text, X_audio)\n",
    "            val_loss += loss_function(output, y).item()\n",
    "    \n",
    "    val_loss /= len(valloader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss:.4f} , Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        X_text, X_audio, y = data\n",
    "        X_text = X_text.unsqueeze(1)\n",
    "        X_audio = X_audio.permute(1, 0, 2)\n",
    "        output = F.sigmoid(model(X_text, X_audio))\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "            \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(torch.argmax(y, dim=1).cpu().numpy())\n",
    "\n",
    "        \n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    all_targets = np.array(all_targets).flatten()\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "\n",
    "    print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
