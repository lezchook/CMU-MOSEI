{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from warnings import simplefilter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('Data_Train_modified.csv')\n",
    "data_val = pd.read_csv('Data_Val_modified.csv')\n",
    "data_test = pd.read_csv('Data_Test_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "data_train['sentiment'] = data_train['sentiment'].progress_apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutaral')\n",
    "data_val['sentiment'] = data_val['sentiment'].progress_apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutaral')\n",
    "data_test['sentiment'] = data_test['sentiment'].progress_apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutaral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_count = data_train[data_train[\"sentiment\"] == \"neutaral\"].shape[0]\n",
    "data_positive = data_train[data_train[\"sentiment\"] == \"positive\"]\n",
    "data_negative = data_train[data_train[\"sentiment\"] == \"negative\"]\n",
    "\n",
    "data_positive_downsampled = resample(data_positive, replace=False, n_samples=neutral_count, random_state=42)\n",
    "data_negative_downsampled = resample(data_negative, replace=False, n_samples=neutral_count, random_state=42)\n",
    "data_train = pd.concat([data_train[data_train[\"sentiment\"] == \"neutaral\"], data_positive_downsampled, data_negative_downsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.reset_index().drop(columns=[\"index\"])\n",
    "data_val = data_val.reset_index().drop(columns=[\"index\"])\n",
    "data_test = data_test.reset_index().drop(columns=[\"index\"])\n",
    "\n",
    "data_train = data_train.rename(columns={\"video\": \"audio_file\"})\n",
    "data_val = data_val.rename(columns={\"video\": \"audio_file\"})\n",
    "data_test = data_test.rename(columns={\"video\": \"audio_file\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folder = \"./Audio/WAV_16000/\"\n",
    "SAMPLE_RATE = 16000\n",
    "HOP_LENGTH = 512\n",
    "N_MFCC= 13\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path, start_time, end_time, n_mfcc):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    start_sample = int(start_time * sr)\n",
    "    end_sample = int(end_time * sr)\n",
    "    y_segment = y[start_sample:end_sample]\n",
    "        \n",
    "    mfcc = librosa.feature.mfcc(y=y_segment, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc_mean = np.mean(mfcc.T, axis=0)   \n",
    "    return mfcc_mean.flatten()\n",
    "\n",
    "def extract_pitch(file_path, start_time, end_time):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    start_sample = int(start_time * sr)\n",
    "    end_sample = int(end_time * sr)\n",
    "    y_segment = y[start_sample:end_sample]\n",
    "    \n",
    "    pitch, _ = librosa.core.piptrack(y=y_segment, sr=sr)\n",
    "    return np.mean(pitch)\n",
    "\n",
    "def extract_bert_features(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=600).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(data, n_mfcc):\n",
    "    data[\"mfcc\"] = data.progress_apply(lambda row: extract_mfcc(os.path.join(audio_folder, row[\"audio_file\"] + \".wav\"), row[\"start_time\"], row[\"end_time\"], N_MFCC), axis=1)\n",
    "    mfcc_columns = [f\"mfcc_{i}\" for i in range(n_mfcc)]\n",
    "    data[mfcc_columns] = pd.DataFrame(data[\"mfcc\"].tolist(), index=data.index)\n",
    "    data.drop(columns=[\"mfcc\"], inplace=True)\n",
    "    return data\n",
    "\n",
    "def get_pitch(data):\n",
    "    data[\"pitch\"] = data.progress_apply(lambda row: extract_pitch(os.path.join(audio_folder, row[\"audio_file\"] + \".wav\"), row[\"start_time\"], row[\"end_time\"]), axis=1)\n",
    "    return data\n",
    "\n",
    "def get_bert_embedings(data):\n",
    "    simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "    data[\"bert_features\"] = data[\"text\"].progress_apply(lambda x: extract_bert_features(x))\n",
    "    bert_columns = [f\"bert_{i}\" for i in range(768)]\n",
    "    data[bert_columns] = pd.DataFrame(data[\"bert_features\"].tolist(), index=data.index)\n",
    "    data.drop(columns=[\"bert_features\"], inplace=True)\n",
    "    return data\n",
    "\n",
    "def get_tfidf(vectorizer, data):\n",
    "    tfidf = vectorizer.transform(data[\"ASR\"])\n",
    "    df_tfidf = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return pd.concat([data, df_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train = get_mfcc(data_train, N_MFCC)\n",
    "#data_val = get_mfcc(data_val, N_MFCC)\n",
    "#data_test = get_mfcc(data_test, N_MFCC)\n",
    "\n",
    "#data_train = get_pitch(data_train)\n",
    "#data_val = get_pitch(data_val)\n",
    "#data_test = get_pitch(data_test)\n",
    "\n",
    "data_train = get_bert_embedings(data_train)\n",
    "data_val = get_bert_embedings(data_val)\n",
    "#data_test = get_bert_embedings(data_test)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "vectorizer.fit(data_train[\"ASR\"])\n",
    "data_train = get_tfidf(vectorizer, data_train)\n",
    "data_val = get_tfidf(vectorizer, data_val)\n",
    "#data_test = get_tfidf(vectorizer, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.concat([data_train, pd.get_dummies(data_train['sentiment'])], axis=1)\n",
    "data_val = pd.concat([data_val, pd.get_dummies(data_val['sentiment'])], axis=1)\n",
    "data_test = pd.concat([data_test, pd.get_dummies(data_test['sentiment'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.drop(['audio_file', 'start_time', 'end_time', 'sentiment', 'happy', 'sad',\t'anger', 'surprise', 'disgust',\t'fear', 'text', 'ASR', 'negative', 'neutaral', 'positive'], axis=1)\n",
    "y_train = data_train[['negative', 'neutaral', 'positive']]\n",
    "\n",
    "X_val = data_val.drop(['audio_file', 'start_time', 'end_time', 'sentiment', 'happy', 'sad',\t'anger', 'surprise', 'disgust',\t'fear', 'text', 'ASR', 'negative', 'neutaral', 'positive'], axis=1)\n",
    "y_val = data_val[['negative', 'neutaral', 'positive']]\n",
    "\n",
    "X_test = data_test.drop(['audio_file', 'start_time', 'end_time', 'sentiment', 'happy', 'sad',\t'anger', 'surprise', 'disgust',\t'fear', 'text', 'ASR', 'negative', 'neutaral', 'positive'], axis=1)\n",
    "y_test = data_test[['negative', 'neutaral', 'positive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, device):\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index], self.targets[index]\n",
    "\n",
    "class DeepNeuralAudio(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_shape[1], 1024)\n",
    "        self.linear2 = nn.Linear(1024, 1024)\n",
    "        self.linear3 = nn.Linear(1024, 256)\n",
    "        self.linear4 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        return x\n",
    "    \n",
    "class DeepNeural(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_shape[1], 512)\n",
    "        self.linear2 = nn.Linear(512, 512)\n",
    "        self.linear3 = nn.Linear(512, 64)\n",
    "        self.linear4 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "trainset = CustomDataset(X_train.to_numpy(), y_train.to_numpy(), device)\n",
    "trainloader = DataLoader(trainset, batch_size=4096, shuffle=True)\n",
    "\n",
    "valset = CustomDataset(X_val.to_numpy(), y_val.to_numpy(), device)\n",
    "valloader = DataLoader(valset, batch_size=4096, shuffle=False)\n",
    "\n",
    "testset = CustomDataset(X_test.to_numpy(), y_test.to_numpy(), device)\n",
    "testloader = DataLoader(testset, batch_size=256, shuffle=False)\n",
    "\n",
    "model = DeepNeural(X_train.shape, y_train.shape[1]).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(160):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data in trainloader:\n",
    "        X, y = data\n",
    "        output = model(X)\n",
    "        loss = loss_function(output, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(trainloader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            X, y = data\n",
    "            output = model(X)\n",
    "            val_loss += loss_function(output, y).item()\n",
    "    \n",
    "    val_loss /= len(valloader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss:.4f} , Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_f1(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            X, y = data\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(torch.argmax(y, dim=1).cpu().numpy())\n",
    "\n",
    "        \n",
    "        all_preds = np.array(all_preds).flatten()\n",
    "        all_targets = np.array(all_targets).flatten()\n",
    "        f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "\n",
    "        print(\"F1-score:\", f1)\n",
    "\n",
    "evaluate_f1(model, valloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
